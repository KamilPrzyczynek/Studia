# -*- coding: utf-8 -*-
"""lab2zum2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4xGa_y87zim5eRCZBSZZyDAZDewJlIx

Metody redukcji wymiarowości w modelach regresji liniowej
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge
from sklearn import linear_model
from sklearn.feature_selection import RFE

# Zadanie 1: Wczytanie i analiza danych
url = "https://gist.githubusercontent.com/aishwarya8615/89d9f36fc014dea62487f7347864d16a/raw/8629d284e13976dcb13bb0b27043224b9266fffa/Life_Expectancy_Data.csv"

try:
    data = pd.read_csv(url)
    print(data.head())
except Exception as e:
    print(f"Błąd podczas wczytywania danych: {e}")

# Czyszczenie danych
data_cleaned = data.replace('Unknown', np.nan).dropna()

# Obliczenie korelacji
correlation_matrix = data_cleaned.iloc[:, 4:23].corr(method='pearson')

# Wizualizacja korelacji za pomocą mapy ciepła
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Heatmap korelacji między czynnikami")
plt.show()

# Przygotowanie danych do modelu
X = data_cleaned.iloc[:, 5:23]
y = data_cleaned.iloc[:, 4]

# Wyświetlenie pierwszych wierszy X i y
print(X.head())
print(y.head())

# Budowa modelu regresji liniowej
reg = LinearRegression().fit(X, y)
print("Model R2 is:", reg.score(X, y))

# Regresja grzbietowa
ridge = Ridge(alpha=0.5)
ridge.fit(X, y)
num_selected_features = np.sum(ridge.coef_ != 0)
print("Model R2 is:", ridge.score(X, y))
print("Number of features selected:", num_selected_features)
print("Ridge coefficients:", ridge.coef_)

# Regresja Lasso
lasso = linear_model.Lasso(alpha=0.5)
lasso.fit(X, y)
num_selected_features = np.sum(lasso.coef_ != 0)
print("Model R2 is:", lasso.score(X, y))
print("Number of features selected:", num_selected_features)
print("Lasso coefficients:", lasso.coef_)

# Zadanie 2: Iteracyjna analiza RFE
r2_scores = []
n_features_range = range(1, 19)
for i in n_features_range:
    rfe = RFE(LinearRegression(), n_features_to_select=i)
    fit_rfe = rfe.fit(X, y)
    r2_scores.append(fit_rfe.score(X, y))
    print("Model R2 is:", fit_rfe.score(X, y))
    print("Number of features selected:", i)

# Wykres wartości R2 względem liczby cech
plt.figure(figsize=(10, 6))
plt.plot(n_features_range, r2_scores, marker='o', linestyle='-')
plt.xlabel("Liczba cech")
plt.ylabel("Wartość R2")
plt.title("Przebieg wartości R2 w zależności od liczby cech")
plt.grid()
plt.show()

# Zadanie 3: Identyfikacja wybranych cech w poszczególnych iteracjach
selected_features_dict = {}
for i in n_features_range:
    rfe = RFE(LinearRegression(), n_features_to_select=i)
    fit_rfe = rfe.fit(X, y)
    selected_features_dict[i] = list(X.columns[fit_rfe.support_])

# Wyświetlenie wybranych cech dla każdej iteracji
for key, value in selected_features_dict.items():
    print(f"Liczba cech: {key}, Wybrane cechy: {value}")

"""3 Metody redukcji wymiarowości w modelach klasyfikacji"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import RFE, SelectKBest, f_classif
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

url = 'https://raw.githubusercontent.com/pkmklong/Breast-Cancer-Wisconsin-Diagnostic-DataSet/master/data.csv'
data = pd.read_csv(url)

# Zadanie 1 - Sprawdzenie balansu danych
result_counts = data['diagnosis'].value_counts()
percentages = (result_counts / result_counts.sum()) * 100
ax = result_counts.plot(kind='bar', figsize=(8, 6),
                        title='Breast Cancer Wisconsin', xlabel='Diagnosis', ylabel='Counts')
for i, (v, p) in enumerate(zip(result_counts, percentages)):
    ax.text(i, v + 0.1, f'{p:.2f}%', ha='center', va='bottom')
plt.show()

# Zadanie 4 - Korelacja i heatmap
corr_matrix = data.iloc[:, 2:32].corr(method='pearson')
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Heatmap korelacji')
plt.show()

# Zadanie 5 - Analiza korelacji
# Silne korelacje między cechami mogą prowadzić do multikolineralności i wpływać na stabilność modelu.

# Przygotowanie danych do modelowania
X = data.iloc[:, 2:32]
y = data.iloc[:, 1]

# Podział na zestaw treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Modele bazowe
LR = LogisticRegression(penalty=None, solver='lbfgs', max_iter=500)
kNN = KNeighborsClassifier(n_neighbors=5)

LR.fit(X_train, y_train)
kNN.fit(X_train, y_train)

print("Logistic Regression Accuracy:", accuracy_score(y_test, LR.predict(X_test)))
print("k-Nearest Neighbors Accuracy:", accuracy_score(y_test, kNN.predict(X_test)))

# Eliminacja cech za pomocą RFE
warnings.filterwarnings("ignore", category=ConvergenceWarning)

accuracies = []
for i in range(1, 31):
    rfe = RFE(LR, n_features_to_select=i)
    fit_rfe = rfe.fit(X_train, y_train)
    selected_features = X_train.columns[fit_rfe.support_]
    LR.fit(X_train[selected_features], y_train)
    accuracy = accuracy_score(y_test, LR.predict(X_test[selected_features]))
    accuracies.append(accuracy)

# Zadanie 6 - Wykres przebiegu dokładności
plt.figure(figsize=(10, 6))
plt.plot(range(1, 31), accuracies, marker='o')
plt.title('Przebieg dokładności modelu w zależności od liczby cech')
plt.xlabel('Liczba cech')
plt.ylabel('Dokładność')
plt.grid()
plt.show()

# Maksymalna dokładność
max_accuracy = max(accuracies)
optimal_features = accuracies.index(max(accuracies)) + 1
print(f'Maksymalna dokładność {max(accuracies):.4f} osiągnięta dla {optimal_features} cech.')

# Zadanie 7 - Regresja logistyczna z regularyzacją L1 i L2
for penalty in ['l2', 'l1']:
    LR_penalty = LogisticRegression(penalty=penalty, solver='saga', max_iter=5000)
    LR_penalty.fit(X_train, y_train)
    accuracy_penalty = accuracy_score(y_test, LR_penalty.predict(X_test))
    print(f"Dokładność regresji logistycznej z regularyzacją {penalty.upper()}: {accuracy_penalty:.4f}")

# Zadanie 8 - Selekcja cech SelectKBest dla kNN
accuracies_kNN = []
for i in range(1, 31):
    selector = SelectKBest(f_classif, k=i)
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)
    kNN.fit(X_train_selected, y_train)
    accuracy = accuracy_score(y_test, kNN.predict(X_test_selected))
    accuracies_kNN.append(accuracy)
    print(f"kNN Accuracy (using {i} features): {accuracy:.4f}")

# Zadanie 9 - Selekcja cech dla modelu Random Forest
accuracies_rf = []
for i in range(1, 31):
    selector = SelectKBest(f_classif, k=i)
    X_train_sel = selector.fit_transform(X_train, y_train)
    X_test_sel = selector.transform(X_test)
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_train_sel, y_train)
    accuracy_rf = accuracy_score(y_test, rf.predict(X_test_sel))
    accuracies_rf.append(accuracy_rf)
    print(f"Random Forest Accuracy (using {i} features): {accuracy_rf:.4f}")