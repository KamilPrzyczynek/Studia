# -*- coding: utf-8 -*-
"""Lab3ZUM2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zfXwiGZvYMz9Cx0Gj1VCD6WFC_DGm8Ed

Analiza głównych składowych (PCA)
"""

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression


df = pd.read_csv("https://raw.githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/refs/heads/master/Wine.csv")
df.head()
df_X = df.drop('Customer_Segment', axis=1)
df_y = df['Customer_Segment']
df_scaled = StandardScaler().fit_transform(df_X)
pca = PCA()
df_pca = pca.fit_transform(df_scaled)

# Procent wariancji wyjasnianej przez kazda glowna skladowa
print('Explained variance ratio:', pca.explained_variance_ratio_)
# Macierz komponentow (wspolczynniki kazdej cechy dla glownych skladowych)
print('Principal components:')
print(pca.components_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_, marker='o', linestyle='--')
plt.xlabel('Liczba skladowych')
plt.ylabel('Wartosc wlasna')
plt.title('Wykres osypiska')
plt.show()
# Procent wyjasnionej wariancji
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = explained_variance_ratio.cumsum()
for i, (ev, cv) in enumerate(zip(explained_variance_ratio,cumulative_variance_ratio), start=1):
  print(f'Skladowa {i}: Wyjasniona wariancja = {ev:.4f}, Skumulowana wyjasniona wariancja = {cv:.4f}')


# Przeprowadzenie PCA
pca2 = PCA(n_components=2)
df_pca2 = pca2.fit_transform(df_scaled)
# Wykres punktowy z kolorami odpowiadajacymi klasom
scatter = plt.scatter(df_pca2[:, 0], df_pca2[:, 1], c=df_y, cmap='viridis', edgecolor='k', s=50)
# Opisy osi
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA- 2 Components')
# Dodanie legendy z trzema klasami
classes = np.unique(df_y)
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor= scatter.cmap(i / 3), markersize=10) for i in range(3)], labels=[f'Klasa {i + 1}' for i in range(3)], title="Klasy")
plt.show()

classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0).fit( df_X, df_y)
classifier.score(df_X, df_y)

classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0).fit( df_pca2, df_y)
classifier.score(df_pca2, df_y)

"""Zadanie 1 : Otrzymany wynik 96%  jest bardzo dobdry ,redukcja wymiarowości za pomocą pca do tylko dwóch komponentów nie spowodowała znacznego spadku jakości predykcji.na wykresie widać idelanie 2 klastry odzielone od siebie"""

pca3 = PCA(n_components=3)
df_pca3 = pca3.fit_transform(df_scaled)
# Tworzenie wykresu 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
# Rysowanie punktow z kolorami odpowiadajacymi klasom
scatter = ax.scatter(df_pca3[:, 0], df_pca3[:, 1], df_pca3[:, 2], c=df_y, cmap='viridis', edgecolor='k', s=50)
# Opisy osi
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
ax.set_title('Wizualizacja PCA w 3D')
# Dodanie legendy
legend = ax.legend(*scatter.legend_elements(), title="Klasy")
ax.add_artist(legend)
plt.show()
classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0).fit( df_pca3, df_y)
classifier.score(df_pca3, df_y)

pca8 = PCA(n_components=8)
df_pca8 = pca8.fit_transform(df_scaled)
classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0).fit( df_pca8, df_y)
classifier.score(df_pca8, df_y)


df_scaled = StandardScaler().fit_transform(df_X)

accuracy = 0
n_components = 1

while accuracy < 1.0 and n_components <= df_X.shape[1]:
    pca = PCA(n_components=n_components)
    df_pca = pca.fit_transform(df_scaled)
    classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0)
    classifier.fit(df_pca, df_y)
    accuracy = classifier.score(df_pca, df_y)
    if accuracy < 1.0:
        n_components += 1

print(f"Liczba składowych potrzebnych do uzyskania 100% dokładności: {n_components}")

"""Zadanie 2 Liczba składowych potrzebnych do uzyskania 100% dokładności: 5

3 Liniowa analiza dyskryminacyjna (LDA)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score

data = pd.read_csv('https://raw.githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/refs/heads/master/Wine.csv')
X = data.drop('Customer_Segment', axis=1)
y = data['Customer_Segment']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X_train, y_train)

# Wizualizacja wynikow LDA w 2D, dla 3 klas
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=50)
 # Opisy osi
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('LDA- 2 Components (3 Classes)')
# Dodanie paska kolorow
plt.colorbar(scatter, label="Customer Segment")
# Dodanie legendy dla 3 klas
classes = np.unique(y_train)
legend_handles = [plt.Line2D([0], [0], marker='o', color='w',
                             markerfacecolor=scatter.cmap(i / len(classes)), markersize=10)
                  for i in range(len(classes))]

ax.legend(handles=legend_handles, labels=[f'Klasa {i + 1}' for i in range(len(classes))], title="Klasy")

plt.grid()
plt.show()

# Predykcja na zbiorze testowym
X_test_lda = lda.transform(X_test)
y_pred = lda.predict(X_test)
# Ocena modelu accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

classifier = LogisticRegression(penalty=None, max_iter=500, random_state=0).fit( X_train, y_train)
classifier.score(X_test, y_test)

"""zadanie 3. Wykonaj analogiczną analizę dla QDA. Czy zastosowanie QDA poprawiło wynik klasyfikacji? Jeśli tak to dlaczego?"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Wczytanie danych
df = pd.read_csv("https://raw.githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/refs/heads/master/Wine.csv")

# Podział danych
df_X = df.drop('Customer_Segment', axis=1)
df_y = df['Customer_Segment']

# Skalowanie danych
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_X)

# Podział danych na treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X_scaled, df_y, test_size=0.2, random_state=0)

# Wykonanie QDA
qda = QDA()
qda_model = qda.fit(X_train, y_train)
y_pred_qda = qda.predict(X_test)

# Ocena dokładności
accuracy_qda = accuracy_score(y_test, y_pred_qda)
print(f"QDA Accuracy: {accuracy_score(y_test, y_pred_qda):.4f}")

# Porównanie z LDA
lda = LDA()
lda.fit(X_train, y_train)
y_pred_lda = lda.predict(X_test)
accuracy_lda = accuracy_score(y_test, y_pred_lda)

print(f"Accuracy LDA: {accuracy_lda:.4f}")
print(f"Accuracy QDA: {accuracy_score(y_test, y_pred_qda):.4f}")

if accuracy_score(y_test, y_pred_qda) > accuracy_lda:
    print("QDA osiągnęło wyższą dokładność niż LDA.")
elif accuracy_score(y_test, y_pred_qda) == accuracy_lda:
    print("QDA i LDA osiągnęły taką samą dokładność.")
else:
    print("LDA osiągnęło wyższą dokładność niż QDA.")

"""4 t-SNE"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Wczytanie danych
url = "https://raw.githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/refs/heads/master/Wine.csv"
data = pd.read_csv(url)

# Podział danych na cechy i etykiety klas
X = data.drop('Customer_Segment', axis=1)
y = data['Customer_Segment']

# Standaryzacja danych
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Podział na zbiór treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)

# t-SNE z dwoma komponentami dla wizualizacji 2D
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X_train)

# Wizualizacja wyników t-SNE
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=50)

# Opisy osi
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.title('t-SNE - 2 Components (3 Classes)')

# Dodanie paska kolorów
plt.colorbar(scatter, label='Customer Segment')

# Dodanie legendy
classes = np.unique(y_train)
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w',
                               markerfacecolor=scatter.cmap(i / len(classes)),
                               markersize=10, markeredgecolor='k')
                    for i in range(len(classes))],
           labels=[f'Klasa {i}' for i in classes],
           title="Klasy")

# Wyświetlenie wykresu
plt.show()

# Analiza trzech klas po t-SNE
for klasa in np.unique(y_train):
    idx = (y_train == klasa)
    x_class = X_tsne[y_train == klasa, :]

    # Wizualizacja dla każdej klasy oddzielnie
    plt.scatter(X_tsne[y_train == klasa, 0], X_tsne[y_train == klasa, 1], label=f'Klasa {klasa}')

    # Obliczenie statystyk położenia klas
    mean_x, mean_y = np.mean(X_tsne[y_train == klasa, :], axis=0)
    std_x, std_y = np.std(X_tsne[y_train == klasa, 0]), np.std(X_tsne[y_train == klasa, 1])

    print(f"\nKlasa {klasa}:")
    print(f" Średnia X: {mean_x:.3f}, Średnia Y: {mean_y:.3f}")
    print(f" Odchylenie standardowe X: {std_x:.3f}, Y: {std_y:.3f}")
    print(f" Liczba punktów w klasie: {len(X_tsne[y_train == klasa])}")

plt.xlabel('t-SNE komponent 1')
plt.ylabel('t-SNE Component 2')
plt.title('Wizualizacja klas po t-SNE')
plt.legend()
plt.grid()
plt.show()

"""Takie wyniki pokazują, że algorytmy nieliniowe ( QDA) mogą działać lepiej niż liniowe, gdy klasy mają różną strukturę rozproszenia danych."""