{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjYa45U1DvN",
        "outputId": "5f16c281-98cc-4276-f9a6-e3cf385f4a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
            "0  75.0        0                       582         0                 20   \n",
            "1  55.0        0                      7861         0                 38   \n",
            "2  65.0        0                       146         0                 20   \n",
            "3  50.0        1                       111         0                 20   \n",
            "4  65.0        1                       160         1                 20   \n",
            "\n",
            "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
            "0                    1  265000.00               1.9           130    1   \n",
            "1                    0  263358.03               1.1           136    1   \n",
            "2                    0  162000.00               1.3           129    1   \n",
            "3                    0  210000.00               1.9           137    1   \n",
            "4                    0  327000.00               2.7           116    0   \n",
            "\n",
            "   smoking  time  DEATH_EVENT  \n",
            "0        0     4            1  \n",
            "1        0     6            1  \n",
            "2        1     7            1  \n",
            "3        0     7            1  \n",
            "4        0     8            1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
        "\n",
        "# Wczytanie danych\n",
        "url = \"https://raw.githubusercontent.com/lorenzodenisi/Heart-Failure-Clinical-Records/refs/heads/master/heart_failure_clinical_records_dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "\n",
        "# Podział na cechy (X) i etykiety (y)\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Podział na zbiór treningowy i testowy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " zad 1 kluczowa jest relacja między poszczególnymi wartościami cech, a nie ich bezwzględna skala. Drzewo decyzyjne dokonuje podziałów danych na podstawie warunków typu „czy wartość cechy jest większa/mniejsza od pewnej liczby”, co oznacza, że kolejność oraz relacja między wartościami są zachowane, niezależnie od tego, czy dane są standaryzowane, czy nie. W przeciwieństwie do algorytmów takich jak SVM, które wykorzystują odległości między punktami w przestrzeni"
      ],
      "metadata": {
        "id": "T6jCPR0Y3aJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    # Predykcje\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]  # Prawdopodobieństwo dla klasy 1\n",
        "\n",
        "    # Obliczenia metryk\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Wypisanie wyników\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# AdaBoost\n",
        "adaboost = AdaBoostClassifier(random_state=42)\n",
        "adaboost.fit(X_train, y_train)\n",
        "evaluate_model(adaboost, X_test, y_test, \"AdaBoost\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
        "gradient_boosting.fit(X_train, y_train)\n",
        "evaluate_model(gradient_boosting, X_test, y_test, \"Gradient Boosting\")\n",
        "\n",
        "# XGBoost\n",
        "xgboost = XGBClassifier(random_state=42)\n",
        "xgboost.fit(X_train, y_train)\n",
        "evaluate_model(xgboost, X_test, y_test, \"XGBoost\")\n",
        "\n",
        "# LightGBM\n",
        "lightgbm = LGBMClassifier(random_state=42, verbose=-1)\n",
        "lightgbm.fit(X_train, y_train)\n",
        "evaluate_model(lightgbm, X_test, y_test, \"LightGBM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N8w1M7r3b6Q",
        "outputId": "6f875967-cd1c-41ea-9d9c-280c982b89b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.7667, F1-score: 0.6667, ROC AUC: 0.8640\n",
            "AdaBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.82        35\n",
            "           1       0.82      0.56      0.67        25\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.78      0.74      0.74        60\n",
            "weighted avg       0.78      0.77      0.76        60\n",
            "\n",
            "Gradient Boosting Accuracy: 0.7167, F1-score: 0.5854, ROC AUC: 0.8823\n",
            "Gradient Boosting Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.89      0.78        35\n",
            "           1       0.75      0.48      0.59        25\n",
            "\n",
            "    accuracy                           0.72        60\n",
            "   macro avg       0.73      0.68      0.69        60\n",
            "weighted avg       0.72      0.72      0.70        60\n",
            "\n",
            "XGBoost Accuracy: 0.7667, F1-score: 0.6818, ROC AUC: 0.8823\n",
            "XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82        35\n",
            "           1       0.79      0.60      0.68        25\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.77      0.74      0.75        60\n",
            "weighted avg       0.77      0.77      0.76        60\n",
            "\n",
            "LightGBM Accuracy: 0.7667, F1-score: 0.6818, ROC AUC: 0.8846\n",
            "LightGBM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82        35\n",
            "           1       0.79      0.60      0.68        25\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.77      0.74      0.75        60\n",
            "weighted avg       0.77      0.77      0.76        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "zad 2 Najlepsze wyniki: XGBoost i LightGBM, które uzyskały wyższą precyzję (względnie w f1-score) i nieco lepszy ROC AUC.\n",
        "\n",
        "Różnice między metodami: Różnice są raczej niewielkie – wszystkie metody (poza Gradient Boosting) uzyskały porównywalne wyniki pod względem accuracy. Jednakże, patrząc na f1-score oraz ROC AUC, LightGBM (i XGBoost) mają przewagę, choć poprawa nie jest drastyczna."
      ],
      "metadata": {
        "id": "FLGFtBnY379p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "zad 3 Choć różnice nie są ogromne, modyfikacja hiperparametrów (zwiększenie liczby estymatorów do 300 oraz ustawienie learning_rate na 0.2) przyniosła zauważalną poprawę rezultatów klasyfikacji"
      ],
      "metadata": {
        "id": "_Zvifnm_6zld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modified_adaboost = AdaBoostClassifier(random_state=42, n_estimators=300, learning_rate=0.2)\n",
        "modified_adaboost.fit(X_train, y_train)\n",
        "print(\"\\nModified AdaBoost:\")\n",
        "evaluate_model(modified_adaboost, X_test, y_test, \"Modified AdaBoost\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF9Wfv_K4Hg4",
        "outputId": "94f11338-ecd1-4f58-824f-cd3668ea42ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modified AdaBoost:\n",
            "Modified AdaBoost Accuracy: 0.8000, F1-score: 0.7000, ROC AUC: 0.8674\n",
            "Modified AdaBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.97      0.85        35\n",
            "           1       0.93      0.56      0.70        25\n",
            "\n",
            "    accuracy                           0.80        60\n",
            "   macro avg       0.84      0.77      0.77        60\n",
            "weighted avg       0.83      0.80      0.79        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "zad 4 ✓ Zadanie 4. Zmodyfikuj hiperparamtery innych modeli. Spróbuj dopasować wartości hiperparametrów aby zmaksymalizować wartości współczynników jakości modelu. Jakie wyniki udało Ci się uzyskać? Dla której metody modelowania drzew wzmacnianych? Porównaj wyniki z innymi osobami w sali."
      ],
      "metadata": {
        "id": "SVMmEPGP66oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuned Gradient Boosting\n",
        "tuned_gradient = GradientBoostingClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3\n",
        ")\n",
        "tuned_gradient.fit(X_train, y_train)\n",
        "evaluate_model(tuned_gradient, X_test, y_test, \"Tuned Gradient Boosting\")\n",
        "\n",
        "# Tuned XGBoost\n",
        "tuned_xgb = XGBClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "tuned_xgb.fit(X_train, y_train)\n",
        "evaluate_model(tuned_xgb, X_test, y_test, \"Tuned XGBoost\")\n",
        "\n",
        "# Tuned LightGBM\n",
        "tuned_lgbm = LGBMClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31\n",
        ")\n",
        "tuned_lgbm.fit(X_train, y_train)\n",
        "evaluate_model(tuned_lgbm, X_test, y_test, \"Tuned LightGBM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEy1ndxD7Rb_",
        "outputId": "414c50ab-721d-4fec-bd56-5eeeadd8f9b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Gradient Boosting Accuracy: 0.7333, F1-score: 0.6190, ROC AUC: 0.8811\n",
            "Tuned Gradient Boosting Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.89      0.79        35\n",
            "           1       0.76      0.52      0.62        25\n",
            "\n",
            "    accuracy                           0.73        60\n",
            "   macro avg       0.74      0.70      0.71        60\n",
            "weighted avg       0.74      0.73      0.72        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:41:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned XGBoost Accuracy: 0.8000, F1-score: 0.7273, ROC AUC: 0.8880\n",
            "Tuned XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84        35\n",
            "           1       0.84      0.64      0.73        25\n",
            "\n",
            "    accuracy                           0.80        60\n",
            "   macro avg       0.81      0.78      0.78        60\n",
            "weighted avg       0.81      0.80      0.79        60\n",
            "\n",
            "Tuned LightGBM Accuracy: 0.7667, F1-score: 0.6818, ROC AUC: 0.8800\n",
            "Tuned LightGBM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.89      0.82        35\n",
            "           1       0.79      0.60      0.68        25\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.77      0.74      0.75        60\n",
            "weighted avg       0.77      0.77      0.76        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuned Gradient Boosting osiąga Accuracy: 0.7333, F1-score: 0.6190 oraz ROC AUC: 0.8811.\n",
        "\n",
        "Tuned XGBoost uzyskuje Accuracy: 0.8000, F1-score: 0.7273 oraz ROC AUC: 0.8880.\n",
        "\n",
        "Tuned LightGBM daje Accuracy: 0.7667, F1-score: 0.6818 oraz ROC AUC: 0.8800.\n",
        "\n",
        "Widać, że najlepsze rezultaty uzyskano dla Tuned XGBoost, który wyróżnia się najwyższym F1-score i"
      ],
      "metadata": {
        "id": "sT_i3w3h7x87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "zad 5 Zadanie 5. Podobne eksperymenty wykonaj dla pliku z większą liczbą klas np.:https://raw. githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/ refs/heads/master/Wine.csv. Aby uporać się z problemem wieloklasowym należy przerobić kod funkcji evaluete model. Poza tym dla klasyfikatora XGBoost należy przerobić zmienną odpowiedzi jeśli etykiety nie zaczynają się od 0. Można porzucić ten model lub spróbować dopisać kod modyfikujący etykietowanie danych w problemie wieloklasowym."
      ],
      "metadata": {
        "id": "fVjcadkA7zS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Wczytanie danych Wine\n",
        "url_wine = \"https://raw.githubusercontent.com/Ayantika22/PCA-Principle-Component-Analysis-For-Wine-dataset/refs/heads/master/Wine.csv\"\n",
        "wine_df = pd.read_csv(url_wine)\n",
        "\n",
        "# Podział na cechy i etykiety\n",
        "X_wine = wine_df.drop('Customer_Segment', axis=1)\n",
        "y_wine = wine_df['Customer_Segment']\n",
        "\n",
        "# Mapowanie etykiet, jeśli nie zaczynają się od 0 (konieczne dla XGBoost)\n",
        "if y_wine.min() != 0:\n",
        "    mapping = {label: idx for idx, label in enumerate(sorted(y_wine.unique()))}\n",
        "    y_wine = y_wine.map(mapping)\n",
        "\n",
        "# Podział danych na zbiór treningowy i testowy\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(X_wine, y_wine, test_size=0.2, random_state=42)\n",
        "\n",
        "def evaluate_model_multiclass(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Obliczenie ROC AUC dla problemu wieloklasowego\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "    except Exception as e:\n",
        "        roc_auc = None\n",
        "        print(\"ROC AUC could not be computed:\", e)\n",
        "\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\", end=\"\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\", ROC AUC: {roc_auc:.4f}\")\n",
        "    else:\n",
        "        print(\"\")\n",
        "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Przykładowy model Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=150, learning_rate=0.1, max_depth=3)\n",
        "gb_model.fit(X_train_wine, y_train_wine)\n",
        "evaluate_model_multiclass(gb_model, X_test_wine, y_test_wine, \"Gradient Boosting (Wine)\")\n",
        "\n",
        "# Przykładowy model XGBoost (po modyfikacji etykiet)\n",
        "xgb_model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "xgb_model.fit(X_train_wine, y_train_wine)\n",
        "evaluate_model_multiclass(xgb_model, X_test_wine, y_test_wine, \"XGBoost (Wine)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y6TnDzE7y5p",
        "outputId": "b626fbd2-89ae-4f30-ae3c-38ff6d717936"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting (Wine) Accuracy: 0.9444, F1-score: 0.9440, ROC AUC: 0.9913\n",
            "Gradient Boosting (Wine) Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       0.93      0.93      0.93        14\n",
            "           2       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.95      0.93      0.94        36\n",
            "weighted avg       0.95      0.94      0.94        36\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:44:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost (Wine) Accuracy: 1.0000, F1-score: 1.0000, ROC AUC: 1.0000\n",
            "XGBoost (Wine) Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      1.00      1.00        14\n",
            "           2       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        36\n",
            "   macro avg       1.00      1.00      1.00        36\n",
            "weighted avg       1.00      1.00      1.00        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model XGBoost osiąga idealne wyniki na zbiorze testowym, co może oznaczać, że model doskonale dopasował się do danych testowych. Jednakże takie wyniki należy interpretować ostrożnie – istnieje ryzyko przeuczenia (overfittingu), szczególnie jeśli zbiór testowy jest niewielki (w tym przypadku 36 próbek).\n",
        "\n",
        "Podsumowując, mimo że XGBoost uzyskał perfekcyjne wyniki, warto zwrócić uwagę na wielkość zbioru testowego oraz ewentualną potrzebę walidacji krzyżowej, aby upewnić się, że rezultaty są stabilne."
      ],
      "metadata": {
        "id": "XUGI7fv18cFX"
      }
    }
  ]
}